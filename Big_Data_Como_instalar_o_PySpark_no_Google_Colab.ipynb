{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardoml93/PySpark-no-Google-Colab/blob/main/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTruZS5TPEvK"
      },
      "source": [
        "# Big Data: Como instalar o PySpark no Google Colab\n",
        "\n",
        "Como instalar o PySpark no Google Colab é uma dúvida comum entre aqueles que estão migrando seus projetos de Data Science para ambientes na nuvem.\n",
        "\n",
        "O termo Big Data está cada vez mais presente, e mesmo projetos pessoais podem assumir uma grande dimensionalidade devido à quantidade de dados disponíveis.\n",
        "\n",
        "Para analisar grandes volumes de dados, Big Data, com velocidade, o Apache Spark é uma ferramenta muito utilizada, dada a sua capacidade de processamento de dados e computação paralela.\n",
        "\n",
        "O Spark foi pensado para ser acessível, oferecendo diversas APIs e frameworks em Python, Scala, SQL e diversas outras linguagens.\n",
        "\n",
        "## PySpark no Google Colab\n",
        "\n",
        "PySpark é a interface alto nível que permite você conseguir acessar e usar o Spark por meio da linguagem Python. Usando o PySpark, você consegue escrever todo o seu código usando apenas o nosso estilo Python de escrever código.\n",
        "\n",
        "Já o Google Colab é uma ferramenta incrível, poderosa e gratuita – com suporte de GPU inclusive. Uma vez que roda 100% na nuvem, você não tem a necessidade de instalar qualquer coisa na sua própria máquina.\n",
        "\n",
        "No entanto, apesar da maioria das bibliotecas de Data Science estarem previamente instaladas no Colab, o mesmo não acontece com o PySpark. Para conseguir usar o PySpark é necessário alguns passos intermediários, que não são triviais para aqueles que estão começando.\n",
        "\n",
        "Dessa maneira, preparei um tutorial simples e direto ensinando a instalar as dependências e a biblioteca.\n",
        "\n",
        "## Instalando o PySpark no Google Colab\n",
        "\n",
        "Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz_8sWI7PKEl"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabkOXVRPYgq"
      },
      "source": [
        "A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.\n",
        "\n",
        "Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpG11RQPbRf"
      },
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klur5aiRPdqx"
      },
      "source": [
        "Com tudo pronto, vamos rodar uma sessão local para testar se a instalação funcionou corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpJqWggpPKXO",
        "outputId": "6b48f3cf-a65c-493d-bed9-bd022afcf56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# download do http para arquivo local\n",
        "!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "listings.csv.1      100%[===================>]   4.49M  6.59MB/s    in 0.7s    \n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: string (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- minimum_nights: string (nullable = true)\n",
            " |-- number_of_reviews: string (nullable = true)\n",
            " |-- last_review: string (nullable = true)\n",
            " |-- reviews_per_month: string (nullable = true)\n",
            " |-- calculated_host_listings_count: double (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5eswa9ZzWS"
      },
      "source": [
        "## Big Data e Python\n",
        "\n",
        "A biblioteca PySpark permite você criar seu servidor Apache Spark, trabalhar com grandes volumes de dados e até mesmo fazer streaming em tempo real.\n",
        "\n",
        "Na minha opinião, o Spark é o melhor framework para trabalhar com Big Data. Tenha certeza que o PySpark vai te ajudar muito ao criar uma interface Python que permita a comunicação entre seu projeto e o servidor.\n",
        "\n",
        "Neste artigo, o meu objetivo foi unicamente apresentar a biblioteca, além de ensinar como você pode instalá-la em um ambiente de nuvem gratuito, o Google Colab. Aproveite e comece a usar hoje mesmo 🙂"
      ]
    }
  ]
}